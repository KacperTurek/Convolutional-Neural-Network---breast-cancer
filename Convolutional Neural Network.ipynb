{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kacper\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First convolution layer.\n",
    "I decided to use 32 feature maps 3x3 pixels, input shape is the same as original pictures which is 50x50 pixels and there is another dimension because pictures are in RGB scale. To get rid of linearity in the pictures I used ReLU functions (Rectified Linear Unit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kacper\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(50, 50, 3..., activation=\"relu\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Convolution2D(32, 3, 3, input_shape = (50, 50, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second convolution layer.\n",
    "Using one convolutional layer resulted in unsatisfying results which were not very bad but could be better. To achieve that improvement I added anothet convolutional layer, the same as the first one and this action resulted in better accuracy which you will see int the last step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kacper\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flattening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full connection.\n",
    "There is on hidden layer in this fully connected neural network, it consists of 128 neurons and the their activation fucntion is rectifier function. For the output layer I chose sigmoid function which I believe is a good choice for this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kacper\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Kacper\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 128, activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the CNN.\n",
    "Optimizer was chosen to be adam which is one of the stochastic gradient descent algorithms, loss function is binary crossentropy, if there were more than two categories, categorical crossentropy woudl be a better choice, but in this there are two categories: pictures with invasive ductal carcinoma (1) and whitout it (0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the CNN to the images.\n",
    "Code for image preprocessing comes from Keras documentation, it contains everything that is needed for those pictures to be useful for presented convolutional neural network. ImageDataGenerator is used for training set so there would be some variety which prevents overfitting. Just in case of some pictues having different size than 50x50 pixels target_size is set to (50, 50). CNN uses 7042 pictures as trainig set and 1759 as test set, description of these pictures is in README file. Number of epocs equal to 25 is enough to achieve good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7042 images belonging to 2 classes.\n",
      "Found 1759 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kacper\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\Kacper\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., steps_per_epoch=220, epochs=25, validation_steps=1759)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "220/220 [==============================] - 31s 143ms/step - loss: 0.4608 - acc: 0.7943 - val_loss: 0.4279 - val_acc: 0.8130\n",
      "Epoch 2/25\n",
      "220/220 [==============================] - 28s 127ms/step - loss: 0.3775 - acc: 0.8449 - val_loss: 0.3703 - val_acc: 0.8408\n",
      "Epoch 3/25\n",
      "220/220 [==============================] - 30s 136ms/step - loss: 0.3896 - acc: 0.8369 - val_loss: 0.3910 - val_acc: 0.8488\n",
      "Epoch 4/25\n",
      "220/220 [==============================] - 31s 142ms/step - loss: 0.3685 - acc: 0.8487 - val_loss: 0.4096 - val_acc: 0.8306\n",
      "Epoch 5/25\n",
      "220/220 [==============================] - 35s 160ms/step - loss: 0.3463 - acc: 0.8581 - val_loss: 0.3764 - val_acc: 0.8363\n",
      "Epoch 6/25\n",
      "220/220 [==============================] - 35s 157ms/step - loss: 0.3448 - acc: 0.8514 - val_loss: 0.3260 - val_acc: 0.8579\n",
      "Epoch 7/25\n",
      "220/220 [==============================] - 35s 158ms/step - loss: 0.3481 - acc: 0.8529 - val_loss: 0.3572 - val_acc: 0.8533\n",
      "Epoch 8/25\n",
      "220/220 [==============================] - 34s 157ms/step - loss: 0.3447 - acc: 0.8507 - val_loss: 0.3393 - val_acc: 0.8584\n",
      "Epoch 9/25\n",
      "220/220 [==============================] - 34s 155ms/step - loss: 0.3322 - acc: 0.8608 - val_loss: 0.3499 - val_acc: 0.8516\n",
      "Epoch 10/25\n",
      "220/220 [==============================] - 34s 155ms/step - loss: 0.3312 - acc: 0.8584 - val_loss: 0.3200 - val_acc: 0.8641\n",
      "Epoch 11/25\n",
      "220/220 [==============================] - 35s 159ms/step - loss: 0.3221 - acc: 0.8652 - val_loss: 0.3283 - val_acc: 0.8556\n",
      "Epoch 12/25\n",
      "220/220 [==============================] - 34s 156ms/step - loss: 0.3339 - acc: 0.8601 - val_loss: 0.3726 - val_acc: 0.8340\n",
      "Epoch 13/25\n",
      "220/220 [==============================] - 34s 154ms/step - loss: 0.3211 - acc: 0.8670 - val_loss: 0.3408 - val_acc: 0.8619\n",
      "Epoch 14/25\n",
      "220/220 [==============================] - 34s 154ms/step - loss: 0.3143 - acc: 0.8695 - val_loss: 0.3171 - val_acc: 0.8624\n",
      "Epoch 15/25\n",
      "220/220 [==============================] - 34s 154ms/step - loss: 0.3117 - acc: 0.8632 - val_loss: 0.3338 - val_acc: 0.8675\n",
      "Epoch 16/25\n",
      "220/220 [==============================] - 34s 153ms/step - loss: 0.3078 - acc: 0.8726 - val_loss: 0.3442 - val_acc: 0.8499\n",
      "Epoch 17/25\n",
      "220/220 [==============================] - 34s 153ms/step - loss: 0.3162 - acc: 0.8676 - val_loss: 0.3041 - val_acc: 0.8692\n",
      "Epoch 18/25\n",
      "220/220 [==============================] - 35s 161ms/step - loss: 0.3053 - acc: 0.8767 - val_loss: 0.3404 - val_acc: 0.8573\n",
      "Epoch 19/25\n",
      "220/220 [==============================] - 33s 152ms/step - loss: 0.2935 - acc: 0.8751 - val_loss: 0.3235 - val_acc: 0.8641\n",
      "Epoch 20/25\n",
      "220/220 [==============================] - 34s 154ms/step - loss: 0.3214 - acc: 0.8665 - val_loss: 0.3345 - val_acc: 0.8670\n",
      "Epoch 21/25\n",
      "220/220 [==============================] - 35s 159ms/step - loss: 0.2924 - acc: 0.8783 - val_loss: 0.3140 - val_acc: 0.8698\n",
      "Epoch 22/25\n",
      "220/220 [==============================] - 34s 154ms/step - loss: 0.2936 - acc: 0.8770 - val_loss: 0.3259 - val_acc: 0.8715\n",
      "Epoch 23/25\n",
      "220/220 [==============================] - 33s 152ms/step - loss: 0.2910 - acc: 0.8761 - val_loss: 0.3161 - val_acc: 0.8727\n",
      "Epoch 24/25\n",
      "220/220 [==============================] - 34s 154ms/step - loss: 0.2852 - acc: 0.8864 - val_loss: 0.3383 - val_acc: 0.8624\n",
      "Epoch 25/25\n",
      "220/220 [==============================] - 34s 155ms/step - loss: 0.2894 - acc: 0.8774 - val_loss: 0.3628 - val_acc: 0.8471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b4bd58ae10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('data/training_set',\n",
    "                                                 target_size = (50, 50),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('data/test_set',\n",
    "                                            target_size = (50, 50),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         samples_per_epoch = 7042,\n",
    "                         nb_epoch = 25,\n",
    "                         validation_data = test_set,\n",
    "                         nb_val_samples = 1759)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results.\n",
    "After last epoc you can see that accuracy for trainig set is 87.74% and fot test set 84.71%. Both values are above 80% which is a great outcome and they are close to eachother and that provides the information that overfitting did not occur. I could probably obtain higher accuracy by using more pictures and more hidden layers or neurons in the hidden layer but that would increase the needed computing power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
